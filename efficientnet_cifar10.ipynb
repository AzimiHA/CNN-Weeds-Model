{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import urllib.error\n",
    "import torchtext\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import scipy.misc\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar100_stats = ((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "cifar10_stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "transform = transforms.Compose(\n",
    "[transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
    " transforms.ToTensor(),\n",
    "     transforms.Normalize(cifar100_stats[0], cifar10_stats[1])])\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "[transforms.ToTensor(),\n",
    "     transforms.Normalize(cifar100_stats[0], cifar10_stats[1])])\n",
    "batch_size = 256\n",
    "\n",
    "#val_size = 1500\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='C:\\\\Users\\Cena\\PycharmProjects\\csc413 final', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='C:\\\\Users\\Cena\\PycharmProjects\\csc413 final', train=False,\n",
    "                                       download=True, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=200,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to C:\\Users\\Cena/.cache\\torch\\hub\\checkpoints\\efficientnet-b2-8bb594d6.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=36804509.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f903e0220fd44dda60eee5482bd4478"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "PATH = 'C:\\\\Users\\Cena\\PycharmProjects\\csc413 final\\efficientnetb0_0_cifar100.pt'\n",
    "pre_trained = True\n",
    "if pre_trained:\n",
    "    net = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "else:\n",
    "    net = EfficientNet.from_name(\"efficientnet-b2 \")\n",
    "\n",
    "\n",
    "#uncomment these block to load trained model\n",
    "load_model = False\n",
    "if load_model:\n",
    "    state_dict = torch.load(PATH)\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    # load params\n",
    "    net.load_state_dict(new_state_dict)\n",
    "net = net.cuda()\n",
    "\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "#cudnn.benchmark = True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 | Loss: 3.3150501944580855 | Training accuracy: 0.12976 | test loss: 412.65484619140625 | test_acc: 0.1\n",
      "Iteration: 2 | Loss: 2.135742258660647 | Training accuracy: 0.20512 | test loss: 2.2869760990142822 | test_acc: 0.1004\n",
      "Iteration: 3 | Loss: 1.9428852097112306 | Training accuracy: 0.26648 | test loss: 2.463998556137085 | test_acc: 0.1024\n",
      "Iteration: 4 | Loss: 1.8035004941784605 | Training accuracy: 0.32034 | test loss: 2.388169288635254 | test_acc: 0.134\n",
      "Iteration: 5 | Loss: 1.7187806428695211 | Training accuracy: 0.35942 | test loss: 2.5920629501342773 | test_acc: 0.1\n",
      "Iteration: 6 | Loss: 1.647651992282089 | Training accuracy: 0.38316 | test loss: 2.5447309017181396 | test_acc: 0.1388\n",
      "Iteration: 7 | Loss: 1.553493862857624 | Training accuracy: 0.4252 | test loss: 2.603900194168091 | test_acc: 0.1089\n",
      "Iteration: 8 | Loss: 1.4763317752857597 | Training accuracy: 0.45812 | test loss: 2.533982753753662 | test_acc: 0.139\n",
      "Iteration: 9 | Loss: 1.411367590330085 | Training accuracy: 0.48472 | test loss: 2.3152239322662354 | test_acc: 0.1634\n",
      "Iteration: 10 | Loss: 1.3564194665879619 | Training accuracy: 0.51048 | test loss: 2.3371684551239014 | test_acc: 0.1637\n",
      "Iteration: 11 | Loss: 1.2575461383985014 | Training accuracy: 0.54898 | test loss: 2.0756919384002686 | test_acc: 0.1908\n",
      "Iteration: 12 | Loss: 1.1798024134976524 | Training accuracy: 0.57666 | test loss: 2.1890833377838135 | test_acc: 0.2095\n",
      "Iteration: 13 | Loss: 1.110014361082291 | Training accuracy: 0.60184 | test loss: 1.9560192823410034 | test_acc: 0.259\n",
      "Iteration: 14 | Loss: 1.059889631307855 | Training accuracy: 0.6225 | test loss: 2.070157289505005 | test_acc: 0.2547\n",
      "Iteration: 15 | Loss: 1.050506868836831 | Training accuracy: 0.62712 | test loss: 1.752695918083191 | test_acc: 0.3952\n",
      "Iteration: 16 | Loss: 0.9800408123707285 | Training accuracy: 0.65434 | test loss: 1.8043824434280396 | test_acc: 0.397\n",
      "Iteration: 17 | Loss: 0.9197214793185798 | Training accuracy: 0.67556 | test loss: 1.6059753894805908 | test_acc: 0.4513\n",
      "Iteration: 18 | Loss: 0.8745470013545484 | Training accuracy: 0.6907 | test loss: 1.5819107294082642 | test_acc: 0.4123\n",
      "Iteration: 19 | Loss: 0.8341027568189465 | Training accuracy: 0.70662 | test loss: 1.3919705152511597 | test_acc: 0.5598\n",
      "Iteration: 20 | Loss: 0.805798911318487 | Training accuracy: 0.71944 | test loss: 1.258800745010376 | test_acc: 0.5817\n",
      "Iteration: 21 | Loss: 0.7704902607567456 | Training accuracy: 0.73136 | test loss: 1.153926968574524 | test_acc: 0.6113\n",
      "Iteration: 22 | Loss: 0.750705965015353 | Training accuracy: 0.73956 | test loss: 1.109695553779602 | test_acc: 0.6221\n",
      "Iteration: 23 | Loss: 0.7427676481251814 | Training accuracy: 0.7405 | test loss: 0.9657014608383179 | test_acc: 0.6824\n",
      "Iteration: 24 | Loss: 0.7266798165379739 | Training accuracy: 0.74734 | test loss: 1.1868215799331665 | test_acc: 0.5696\n",
      "Iteration: 25 | Loss: 0.7201334125533396 | Training accuracy: 0.74748 | test loss: 0.919059693813324 | test_acc: 0.6798\n",
      "Iteration: 26 | Loss: 0.6802315927889883 | Training accuracy: 0.76384 | test loss: 1.0221343040466309 | test_acc: 0.6566\n",
      "Iteration: 27 | Loss: 0.6713938405927347 | Training accuracy: 0.7681 | test loss: 0.9095860123634338 | test_acc: 0.6995\n",
      "Iteration: 28 | Loss: 0.6672420048592042 | Training accuracy: 0.76866 | test loss: 0.8259267210960388 | test_acc: 0.7274\n",
      "Iteration: 29 | Loss: 0.6590163072153014 | Training accuracy: 0.77182 | test loss: 0.8437573313713074 | test_acc: 0.7253\n",
      "Iteration: 30 | Loss: 0.6486735115854108 | Training accuracy: 0.77578 | test loss: 0.7644984722137451 | test_acc: 0.756\n",
      "Iteration: 31 | Loss: 0.6364492760324965 | Training accuracy: 0.77902 | test loss: 0.7246977090835571 | test_acc: 0.7611\n",
      "Iteration: 32 | Loss: 0.6267346580113683 | Training accuracy: 0.78398 | test loss: 0.6773703694343567 | test_acc: 0.7534\n",
      "Iteration: 33 | Loss: 0.62093272866035 | Training accuracy: 0.78508 | test loss: 0.7232982516288757 | test_acc: 0.7455\n",
      "Iteration: 34 | Loss: 0.6124143702339153 | Training accuracy: 0.78876 | test loss: 0.7644089460372925 | test_acc: 0.7459\n",
      "Iteration: 35 | Loss: 0.6088897105382414 | Training accuracy: 0.79092 | test loss: 0.6275686025619507 | test_acc: 0.7746\n",
      "Iteration: 36 | Loss: 0.5907077728485575 | Training accuracy: 0.796 | test loss: 0.6398904323577881 | test_acc: 0.7787\n",
      "Iteration: 37 | Loss: 0.5856428331866557 | Training accuracy: 0.79962 | test loss: 0.73981773853302 | test_acc: 0.7515\n",
      "Iteration: 38 | Loss: 0.5828803212058787 | Training accuracy: 0.79968 | test loss: 0.6901993751525879 | test_acc: 0.7568\n",
      "Iteration: 39 | Loss: 0.5900538937777889 | Training accuracy: 0.79542 | test loss: 0.7624685168266296 | test_acc: 0.7434\n",
      "Iteration: 40 | Loss: 0.5829983881237556 | Training accuracy: 0.80138 | test loss: 0.7151052951812744 | test_acc: 0.7632\n",
      "Iteration: 41 | Loss: 0.5608759256345885 | Training accuracy: 0.80684 | test loss: 0.6513478755950928 | test_acc: 0.748\n",
      "Iteration: 42 | Loss: 0.5573392578837822 | Training accuracy: 0.8082 | test loss: 0.6396045088768005 | test_acc: 0.7779\n",
      "Iteration: 43 | Loss: 0.5562204035873316 | Training accuracy: 0.8074 | test loss: 0.55742347240448 | test_acc: 0.7993\n",
      "Iteration: 44 | Loss: 0.549474303059432 | Training accuracy: 0.81108 | test loss: 0.6328858733177185 | test_acc: 0.7828\n",
      "Iteration: 45 | Loss: 0.553315921401491 | Training accuracy: 0.81056 | test loss: 0.627581000328064 | test_acc: 0.7896\n",
      "Iteration: 46 | Loss: 0.5361524741260373 | Training accuracy: 0.81572 | test loss: 0.5428339242935181 | test_acc: 0.7893\n",
      "Iteration: 47 | Loss: 0.5222587962539829 | Training accuracy: 0.82006 | test loss: 0.6236075758934021 | test_acc: 0.7739\n",
      "Iteration: 48 | Loss: 0.5249721342507674 | Training accuracy: 0.82024 | test loss: 0.6694098114967346 | test_acc: 0.756\n",
      "Iteration: 49 | Loss: 0.5250127770158709 | Training accuracy: 0.82024 | test loss: 0.5887349247932434 | test_acc: 0.7718\n",
      "Iteration: 50 | Loss: 0.5257418943302972 | Training accuracy: 0.8195 | test loss: 0.5959988236427307 | test_acc: 0.8047\n",
      "Iteration: 51 | Loss: 0.5108874858338006 | Training accuracy: 0.82314 | test loss: 0.577587902545929 | test_acc: 0.7981\n",
      "Iteration: 52 | Loss: 0.5124762900629822 | Training accuracy: 0.82406 | test loss: 0.5675821304321289 | test_acc: 0.8024\n",
      "Iteration: 53 | Loss: 0.5040121046560151 | Training accuracy: 0.82622 | test loss: 0.5230962038040161 | test_acc: 0.8096\n",
      "Iteration: 54 | Loss: 0.5094637432876898 | Training accuracy: 0.8254 | test loss: 0.5516483783721924 | test_acc: 0.8085\n",
      "Iteration: 55 | Loss: 0.4975153394803709 | Training accuracy: 0.82884 | test loss: 0.6597369313240051 | test_acc: 0.7916\n",
      "Iteration: 56 | Loss: 0.4892245353180535 | Training accuracy: 0.8319 | test loss: 0.6649909019470215 | test_acc: 0.7816\n",
      "Iteration: 57 | Loss: 0.4863475883493618 | Training accuracy: 0.83314 | test loss: 0.5419654250144958 | test_acc: 0.8095\n",
      "Iteration: 58 | Loss: 0.4854199752515676 | Training accuracy: 0.83272 | test loss: 0.5206795334815979 | test_acc: 0.8082\n",
      "Iteration: 59 | Loss: 0.47744649238124187 | Training accuracy: 0.83586 | test loss: 0.5560323596000671 | test_acc: 0.8042\n",
      "Iteration: 60 | Loss: 0.4846706121247642 | Training accuracy: 0.8337 | test loss: 0.607868492603302 | test_acc: 0.8044\n",
      "Iteration: 61 | Loss: 0.4658405476686906 | Training accuracy: 0.84076 | test loss: 0.5080394148826599 | test_acc: 0.8263\n",
      "Iteration: 62 | Loss: 0.4652481597601151 | Training accuracy: 0.8398 | test loss: 0.544980525970459 | test_acc: 0.8146\n",
      "Iteration: 63 | Loss: 0.4598662792419901 | Training accuracy: 0.8419 | test loss: 0.5391926765441895 | test_acc: 0.8207\n",
      "Iteration: 64 | Loss: 0.4644212658916201 | Training accuracy: 0.84256 | test loss: 0.6204359531402588 | test_acc: 0.7696\n",
      "Iteration: 65 | Loss: 0.46297980084711193 | Training accuracy: 0.84184 | test loss: 0.46819233894348145 | test_acc: 0.8201\n",
      "Iteration: 66 | Loss: 0.4447555377775309 | Training accuracy: 0.8484 | test loss: 0.5109675526618958 | test_acc: 0.8242\n",
      "Iteration: 67 | Loss: 0.4552426800435903 | Training accuracy: 0.84366 | test loss: 0.5121237635612488 | test_acc: 0.8222\n",
      "Iteration: 68 | Loss: 0.4459125884637541 | Training accuracy: 0.84608 | test loss: 0.48832911252975464 | test_acc: 0.8184\n",
      "Iteration: 69 | Loss: 0.4503187649712271 | Training accuracy: 0.8451 | test loss: 0.47589728236198425 | test_acc: 0.8268\n",
      "Iteration: 70 | Loss: 0.44233236735572623 | Training accuracy: 0.84996 | test loss: 0.5110198855400085 | test_acc: 0.8032\n",
      "Iteration: 71 | Loss: 0.4303897927914347 | Training accuracy: 0.85256 | test loss: 0.4803831875324249 | test_acc: 0.8388\n",
      "Iteration: 72 | Loss: 0.42483208176432824 | Training accuracy: 0.85422 | test loss: 0.49203595519065857 | test_acc: 0.8203\n",
      "Iteration: 73 | Loss: 0.42808965563165896 | Training accuracy: 0.8533 | test loss: 0.51655113697052 | test_acc: 0.8134\n",
      "Iteration: 74 | Loss: 0.4320197572209397 | Training accuracy: 0.85252 | test loss: 0.5177280306816101 | test_acc: 0.8057\n",
      "Iteration: 75 | Loss: 0.4293554070956853 | Training accuracy: 0.8524 | test loss: 0.45098307728767395 | test_acc: 0.8354\n",
      "Iteration: 76 | Loss: 0.41077704271491694 | Training accuracy: 0.85816 | test loss: 0.5438668727874756 | test_acc: 0.8234\n",
      "Iteration: 77 | Loss: 0.40947780279176577 | Training accuracy: 0.85892 | test loss: 0.3896271586418152 | test_acc: 0.8368\n",
      "Iteration: 78 | Loss: 0.412679560026344 | Training accuracy: 0.85856 | test loss: 0.42449015378952026 | test_acc: 0.834\n",
      "Iteration: 79 | Loss: 0.41390109062194824 | Training accuracy: 0.8576 | test loss: 0.5104368329048157 | test_acc: 0.8208\n",
      "Iteration: 80 | Loss: 0.41477270895729257 | Training accuracy: 0.85684 | test loss: 0.5750306844711304 | test_acc: 0.8123\n",
      "Iteration: 81 | Loss: 0.3951801561883518 | Training accuracy: 0.86322 | test loss: 0.4781220257282257 | test_acc: 0.8382\n",
      "Iteration: 82 | Loss: 0.3958289942571095 | Training accuracy: 0.86382 | test loss: 0.5210571885108948 | test_acc: 0.8318\n",
      "Iteration: 83 | Loss: 0.39573789190272896 | Training accuracy: 0.86438 | test loss: 0.5054600834846497 | test_acc: 0.8303\n",
      "Iteration: 84 | Loss: 0.397557264536011 | Training accuracy: 0.86382 | test loss: 0.5449573993682861 | test_acc: 0.8274\n",
      "Iteration: 85 | Loss: 0.39674991718968566 | Training accuracy: 0.86482 | test loss: 0.4898624122142792 | test_acc: 0.8405\n",
      "Iteration: 86 | Loss: 0.3848816989635935 | Training accuracy: 0.86842 | test loss: 0.43533647060394287 | test_acc: 0.8396\n",
      "Iteration: 87 | Loss: 0.3880757982937657 | Training accuracy: 0.86776 | test loss: 0.468122661113739 | test_acc: 0.8321\n",
      "Iteration: 88 | Loss: 0.3825600443750012 | Training accuracy: 0.86698 | test loss: 0.47022655606269836 | test_acc: 0.8337\n",
      "Iteration: 89 | Loss: 0.38007776028647716 | Training accuracy: 0.86752 | test loss: 0.47225555777549744 | test_acc: 0.8357\n",
      "Iteration: 90 | Loss: 0.37952690754009755 | Training accuracy: 0.87106 | test loss: 0.41742226481437683 | test_acc: 0.834\n",
      "Iteration: 91 | Loss: 0.37060075253248215 | Training accuracy: 0.87394 | test loss: 0.3771938383579254 | test_acc: 0.8477\n",
      "Iteration: 92 | Loss: 0.3697717268095941 | Training accuracy: 0.87388 | test loss: 0.4133920967578888 | test_acc: 0.8413\n",
      "Iteration: 93 | Loss: 0.368738159461289 | Training accuracy: 0.87282 | test loss: 0.5462355017662048 | test_acc: 0.8332\n",
      "Iteration: 94 | Loss: 0.37324508811746326 | Training accuracy: 0.87106 | test loss: 0.42464175820350647 | test_acc: 0.8402\n",
      "Iteration: 95 | Loss: 0.3680714739828694 | Training accuracy: 0.8736 | test loss: 0.3989430367946625 | test_acc: 0.8532\n",
      "Iteration: 96 | Loss: 0.3605906052552924 | Training accuracy: 0.8758 | test loss: 0.4452075958251953 | test_acc: 0.8319\n",
      "Iteration: 97 | Loss: 0.35713437556916355 | Training accuracy: 0.87612 | test loss: 0.5289269089698792 | test_acc: 0.8244\n",
      "Iteration: 98 | Loss: 0.3627552296586183 | Training accuracy: 0.87502 | test loss: 0.4445568025112152 | test_acc: 0.8465\n",
      "Iteration: 99 | Loss: 0.3586796054578557 | Training accuracy: 0.87714 | test loss: 0.42655104398727417 | test_acc: 0.8408\n",
      "Iteration: 100 | Loss: 0.356910160837733 | Training accuracy: 0.87752 | test loss: 0.3775531053543091 | test_acc: 0.8596\n",
      "Iteration: 101 | Loss: 0.34376759104886834 | Training accuracy: 0.88168 | test loss: 0.41968604922294617 | test_acc: 0.8518\n",
      "Iteration: 102 | Loss: 0.34731277253250686 | Training accuracy: 0.8792 | test loss: 0.38680174946784973 | test_acc: 0.8415\n",
      "Iteration: 103 | Loss: 0.3457163753254073 | Training accuracy: 0.8808 | test loss: 0.4643155038356781 | test_acc: 0.8467\n",
      "Iteration: 104 | Loss: 0.34793627345744443 | Training accuracy: 0.87994 | test loss: 0.37581074237823486 | test_acc: 0.8536\n",
      "Iteration: 105 | Loss: 0.34703388300781346 | Training accuracy: 0.8813 | test loss: 0.4675653576850891 | test_acc: 0.8473\n",
      "Iteration: 106 | Loss: 0.3322423010760424 | Training accuracy: 0.88612 | test loss: 0.400297075510025 | test_acc: 0.8566\n",
      "Iteration: 107 | Loss: 0.3329785921591885 | Training accuracy: 0.88566 | test loss: 0.3858639895915985 | test_acc: 0.8479\n",
      "Iteration: 108 | Loss: 0.33151367276298754 | Training accuracy: 0.88636 | test loss: 0.4503665268421173 | test_acc: 0.8482\n",
      "Iteration: 109 | Loss: 0.3320454174310577 | Training accuracy: 0.88668 | test loss: 0.3929048180580139 | test_acc: 0.8585\n",
      "Iteration: 110 | Loss: 0.32253534909413784 | Training accuracy: 0.88816 | test loss: 0.4431222975254059 | test_acc: 0.8378\n",
      "Iteration: 111 | Loss: 0.32019699684211184 | Training accuracy: 0.88932 | test loss: 0.39615362882614136 | test_acc: 0.853\n",
      "Iteration: 112 | Loss: 0.32098928443631347 | Training accuracy: 0.88852 | test loss: 0.3847924768924713 | test_acc: 0.8558\n",
      "Iteration: 113 | Loss: 0.3236220510182332 | Training accuracy: 0.88794 | test loss: 0.36585724353790283 | test_acc: 0.8565\n",
      "Iteration: 114 | Loss: 0.3214280512564036 | Training accuracy: 0.88994 | test loss: 0.37978672981262207 | test_acc: 0.847\n",
      "Iteration: 115 | Loss: 0.3259888167436026 | Training accuracy: 0.88664 | test loss: 0.4347333014011383 | test_acc: 0.8446\n",
      "Iteration: 116 | Loss: 0.3068369516760719 | Training accuracy: 0.89402 | test loss: 0.3204472064971924 | test_acc: 0.8662\n",
      "Iteration: 117 | Loss: 0.30601478428864964 | Training accuracy: 0.89384 | test loss: 0.46112343668937683 | test_acc: 0.8482\n",
      "Iteration: 118 | Loss: 0.3110076914332351 | Training accuracy: 0.89232 | test loss: 0.33125656843185425 | test_acc: 0.8556\n",
      "Iteration: 119 | Loss: 0.31259024956700754 | Training accuracy: 0.89194 | test loss: 0.34478050470352173 | test_acc: 0.8582\n",
      "Iteration: 120 | Loss: 0.3115403849859627 | Training accuracy: 0.89274 | test loss: 0.39483121037483215 | test_acc: 0.8559\n",
      "Iteration: 121 | Loss: 0.3001075863686143 | Training accuracy: 0.89566 | test loss: 0.3847016394138336 | test_acc: 0.8596\n",
      "Iteration: 122 | Loss: 0.29966292271808703 | Training accuracy: 0.89616 | test loss: 0.3281101882457733 | test_acc: 0.8643\n",
      "Iteration: 123 | Loss: 0.30045980321509497 | Training accuracy: 0.8959 | test loss: 0.37717562913894653 | test_acc: 0.8633\n",
      "Iteration: 124 | Loss: 0.3002457001379558 | Training accuracy: 0.89664 | test loss: 0.37660732865333557 | test_acc: 0.8573\n",
      "Iteration: 125 | Loss: 0.29593245168121496 | Training accuracy: 0.89666 | test loss: 0.36102208495140076 | test_acc: 0.861\n",
      "Iteration: 126 | Loss: 0.2904470540887239 | Training accuracy: 0.89986 | test loss: 0.3705849349498749 | test_acc: 0.8605\n",
      "Iteration: 127 | Loss: 0.2884866409763998 | Training accuracy: 0.89978 | test loss: 0.30182120203971863 | test_acc: 0.8691\n",
      "Iteration: 128 | Loss: 0.290946469091031 | Training accuracy: 0.89994 | test loss: 0.3367128074169159 | test_acc: 0.8671\n",
      "Iteration: 129 | Loss: 0.28866872510739733 | Training accuracy: 0.9005 | test loss: 0.3294205367565155 | test_acc: 0.8674\n",
      "Iteration: 130 | Loss: 0.2896456771663257 | Training accuracy: 0.89996 | test loss: 0.3918255865573883 | test_acc: 0.8564\n",
      "Iteration: 131 | Loss: 0.28438188356100297 | Training accuracy: 0.9026 | test loss: 0.2963836193084717 | test_acc: 0.8682\n",
      "Iteration: 132 | Loss: 0.28238447025722385 | Training accuracy: 0.90222 | test loss: 0.36019280552864075 | test_acc: 0.8692\n",
      "Iteration: 133 | Loss: 0.2762176916003227 | Training accuracy: 0.90378 | test loss: 0.4013366401195526 | test_acc: 0.8657\n",
      "Iteration: 134 | Loss: 0.2769163312504486 | Training accuracy: 0.904 | test loss: 0.3622051477432251 | test_acc: 0.8672\n",
      "Iteration: 135 | Loss: 0.2810354440339974 | Training accuracy: 0.90302 | test loss: 0.38249725103378296 | test_acc: 0.8634\n",
      "Iteration: 136 | Loss: 0.27331963531216796 | Training accuracy: 0.9044 | test loss: 0.3678717315196991 | test_acc: 0.8678\n",
      "Iteration: 137 | Loss: 0.2727655553239949 | Training accuracy: 0.90674 | test loss: 0.34324777126312256 | test_acc: 0.8572\n",
      "Iteration: 138 | Loss: 0.27275985631407523 | Training accuracy: 0.90588 | test loss: 0.38979947566986084 | test_acc: 0.8617\n",
      "Iteration: 139 | Loss: 0.2704786837709193 | Training accuracy: 0.90568 | test loss: 0.3640885651111603 | test_acc: 0.865\n",
      "Iteration: 140 | Loss: 0.2741675102437029 | Training accuracy: 0.9052 | test loss: 0.38454481959342957 | test_acc: 0.864\n",
      "Iteration: 141 | Loss: 0.2631220361407922 | Training accuracy: 0.90952 | test loss: 0.3043557405471802 | test_acc: 0.8658\n",
      "Iteration: 142 | Loss: 0.2621346841357192 | Training accuracy: 0.9092 | test loss: 0.36207208037376404 | test_acc: 0.8668\n",
      "Iteration: 143 | Loss: 0.2592695937016789 | Training accuracy: 0.90968 | test loss: 0.28554898500442505 | test_acc: 0.863\n",
      "Iteration: 144 | Loss: 0.2650952468417129 | Training accuracy: 0.90812 | test loss: 0.3601798117160797 | test_acc: 0.8617\n",
      "Iteration: 145 | Loss: 0.26692301445469563 | Training accuracy: 0.90684 | test loss: 0.36536502838134766 | test_acc: 0.8658\n",
      "Iteration: 146 | Loss: 0.2579002498211909 | Training accuracy: 0.91174 | test loss: 0.3195980191230774 | test_acc: 0.868\n",
      "Iteration: 147 | Loss: 0.256307768806511 | Training accuracy: 0.91084 | test loss: 0.31405699253082275 | test_acc: 0.8701\n",
      "Iteration: 148 | Loss: 0.2533193296011613 | Training accuracy: 0.91158 | test loss: 0.34239405393600464 | test_acc: 0.8705\n",
      "Iteration: 149 | Loss: 0.25298737742158833 | Training accuracy: 0.91302 | test loss: 0.3576168417930603 | test_acc: 0.869\n",
      "Iteration: 150 | Loss: 0.25510676836176793 | Training accuracy: 0.91082 | test loss: 0.33817562460899353 | test_acc: 0.8636\n",
      "Iteration: 151 | Loss: 0.2491174415514177 | Training accuracy: 0.91368 | test loss: 0.29463881254196167 | test_acc: 0.8749\n",
      "Iteration: 152 | Loss: 0.24698788307759226 | Training accuracy: 0.9139 | test loss: 0.3357853591442108 | test_acc: 0.868\n",
      "Iteration: 153 | Loss: 0.24914389719464342 | Training accuracy: 0.91224 | test loss: 0.3602852523326874 | test_acc: 0.8671\n",
      "Iteration: 154 | Loss: 0.24565893578894285 | Training accuracy: 0.91412 | test loss: 0.33296558260917664 | test_acc: 0.8721\n",
      "Iteration: 155 | Loss: 0.2442817263001082 | Training accuracy: 0.915 | test loss: 0.4210854470729828 | test_acc: 0.8611\n",
      "Iteration: 156 | Loss: 0.23986792017002495 | Training accuracy: 0.91668 | test loss: 0.34549015760421753 | test_acc: 0.8693\n",
      "Iteration: 157 | Loss: 0.23694992506382417 | Training accuracy: 0.91802 | test loss: 0.3431989252567291 | test_acc: 0.8794\n",
      "Iteration: 158 | Loss: 0.2378649085151906 | Training accuracy: 0.91662 | test loss: 0.36450743675231934 | test_acc: 0.874\n",
      "Iteration: 159 | Loss: 0.23711531898196864 | Training accuracy: 0.91756 | test loss: 0.4151688814163208 | test_acc: 0.8618\n",
      "Iteration: 160 | Loss: 0.23491643384403113 | Training accuracy: 0.91804 | test loss: 0.4077741205692291 | test_acc: 0.8742\n",
      "Iteration: 161 | Loss: 0.22952630454484296 | Training accuracy: 0.9205 | test loss: 0.3507751524448395 | test_acc: 0.8691\n",
      "Iteration: 162 | Loss: 0.23187890922536655 | Training accuracy: 0.91978 | test loss: 0.33538195490837097 | test_acc: 0.8766\n",
      "Iteration: 163 | Loss: 0.23178118284867735 | Training accuracy: 0.9195 | test loss: 0.35028377175331116 | test_acc: 0.8717\n",
      "Iteration: 164 | Loss: 0.23115571823959447 | Training accuracy: 0.91946 | test loss: 0.3494543433189392 | test_acc: 0.8728\n",
      "Iteration: 165 | Loss: 0.22942779106753214 | Training accuracy: 0.919 | test loss: 0.38823550939559937 | test_acc: 0.8675\n",
      "Iteration: 166 | Loss: 0.2279226959359889 | Training accuracy: 0.92128 | test loss: 0.32117828726768494 | test_acc: 0.8688\n",
      "Iteration: 167 | Loss: 0.22560240845290983 | Training accuracy: 0.9211 | test loss: 0.3333091735839844 | test_acc: 0.8755\n",
      "Iteration: 168 | Loss: 0.22446829118594833 | Training accuracy: 0.9228 | test loss: 0.32924920320510864 | test_acc: 0.8764\n",
      "Iteration: 169 | Loss: 0.22296681224691625 | Training accuracy: 0.9231 | test loss: 0.3327268362045288 | test_acc: 0.8793\n",
      "Iteration: 170 | Loss: 0.22340007308794527 | Training accuracy: 0.92252 | test loss: 0.3902996778488159 | test_acc: 0.873\n",
      "Iteration: 171 | Loss: 0.22116729883211 | Training accuracy: 0.92334 | test loss: 0.3641397953033447 | test_acc: 0.8731\n",
      "Iteration: 172 | Loss: 0.21537023995603835 | Training accuracy: 0.92578 | test loss: 0.40074536204338074 | test_acc: 0.8709\n",
      "Iteration: 173 | Loss: 0.21599933736938604 | Training accuracy: 0.92488 | test loss: 0.3334003686904907 | test_acc: 0.8758\n",
      "Iteration: 174 | Loss: 0.2188458994639163 | Training accuracy: 0.9232 | test loss: 0.33098462224006653 | test_acc: 0.8724\n",
      "Iteration: 175 | Loss: 0.21652947191377075 | Training accuracy: 0.92486 | test loss: 0.3835887908935547 | test_acc: 0.878\n",
      "Iteration: 176 | Loss: 0.21454357637130483 | Training accuracy: 0.92526 | test loss: 0.35450366139411926 | test_acc: 0.8743\n",
      "Iteration: 177 | Loss: 0.20980652467328675 | Training accuracy: 0.92788 | test loss: 0.3480084240436554 | test_acc: 0.8781\n",
      "Iteration: 178 | Loss: 0.21153305851075113 | Training accuracy: 0.92702 | test loss: 0.34547698497772217 | test_acc: 0.8749\n",
      "Iteration: 179 | Loss: 0.2123568526336125 | Training accuracy: 0.92616 | test loss: 0.3051632046699524 | test_acc: 0.8712\n",
      "Iteration: 180 | Loss: 0.21207335758574156 | Training accuracy: 0.92552 | test loss: 0.3543635308742523 | test_acc: 0.8732\n",
      "Iteration: 181 | Loss: 0.2057822643190014 | Training accuracy: 0.92908 | test loss: 0.34687870740890503 | test_acc: 0.8763\n",
      "Iteration: 182 | Loss: 0.2032146718809191 | Training accuracy: 0.92886 | test loss: 0.3318205773830414 | test_acc: 0.8722\n",
      "Iteration: 183 | Loss: 0.20226156605141504 | Training accuracy: 0.92958 | test loss: 0.35795509815216064 | test_acc: 0.8713\n",
      "Iteration: 184 | Loss: 0.2082688564776766 | Training accuracy: 0.92772 | test loss: 0.34500110149383545 | test_acc: 0.8762\n",
      "Iteration: 185 | Loss: 0.20664896329446714 | Training accuracy: 0.9285 | test loss: 0.3259439766407013 | test_acc: 0.8797\n",
      "Iteration: 186 | Loss: 0.2004200565647714 | Training accuracy: 0.93012 | test loss: 0.3357066214084625 | test_acc: 0.876\n",
      "Iteration: 187 | Loss: 0.20385987505468786 | Training accuracy: 0.93024 | test loss: 0.306443989276886 | test_acc: 0.8775\n",
      "Iteration: 188 | Loss: 0.20553424931606468 | Training accuracy: 0.9292 | test loss: 0.400374174118042 | test_acc: 0.8762\n",
      "Iteration: 189 | Loss: 0.19837345034644313 | Training accuracy: 0.93016 | test loss: 0.4073038101196289 | test_acc: 0.8767\n",
      "Iteration: 190 | Loss: 0.19379032486859632 | Training accuracy: 0.93216 | test loss: 0.3679615259170532 | test_acc: 0.8763\n",
      "Iteration: 191 | Loss: 0.19207632632888094 | Training accuracy: 0.9336 | test loss: 0.33738449215888977 | test_acc: 0.8802\n",
      "Iteration: 192 | Loss: 0.19313009007244694 | Training accuracy: 0.93372 | test loss: 0.34317728877067566 | test_acc: 0.8774\n",
      "Iteration: 193 | Loss: 0.19899170678489062 | Training accuracy: 0.9299 | test loss: 0.37438103556632996 | test_acc: 0.8793\n",
      "Iteration: 194 | Loss: 0.19358817476551143 | Training accuracy: 0.93254 | test loss: 0.34391796588897705 | test_acc: 0.8779\n",
      "Iteration: 195 | Loss: 0.19152590480385995 | Training accuracy: 0.93328 | test loss: 0.3245548903942108 | test_acc: 0.879\n",
      "Iteration: 196 | Loss: 0.18709827815087474 | Training accuracy: 0.93584 | test loss: 0.34569239616394043 | test_acc: 0.8768\n",
      "Iteration: 197 | Loss: 0.18997974853430474 | Training accuracy: 0.9348 | test loss: 0.34320542216300964 | test_acc: 0.8785\n",
      "Iteration: 198 | Loss: 0.19044817470926412 | Training accuracy: 0.93346 | test loss: 0.3492962121963501 | test_acc: 0.8751\n",
      "Iteration: 199 | Loss: 0.18936262857549044 | Training accuracy: 0.93366 | test loss: 0.36503952741622925 | test_acc: 0.879\n",
      "Iteration: 200 | Loss: 0.18849911137807127 | Training accuracy: 0.93442 | test loss: 0.3425336182117462 | test_acc: 0.8777\n",
      "==> Finished Training ...\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "loss_list = []\n",
    "test_acc_list = []\n",
    "lrSchduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        #inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Normalizing the loss by the total number of train batches\n",
    "    running_loss /= len(trainloader)\n",
    "\n",
    "    # Calculate training accuracy of the existing model\n",
    "\n",
    "    train_accuracy = correct / total\n",
    "    loss_list.append(running_loss)\n",
    "\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            test_images, test_labels = data\n",
    "            test_inputs = test_images.cuda()\n",
    "            test_labels = test_labels.cuda()\n",
    "            test_outputs = net(test_inputs)\n",
    "            test_loss = criterion(test_outputs, test_labels)\n",
    "            _, predicted = torch.max(test_outputs.data, 1)\n",
    "            test_total += test_labels.size(0)\n",
    "            test_correct += (predicted == test_labels).sum().item()\n",
    "    test_acc = test_correct / test_total\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "    print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2} | test loss: {3} | test_acc: {4}\".format(epoch+1,\n",
    "                                                                                              running_loss,\n",
    "                                                                                              train_accuracy, test_loss,\n",
    "                                                                                                   test_acc\n",
    "                                                                                            ))\n",
    "    lrSchduler.step()\n",
    "save_model = False\n",
    "if save_model:\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "print('==> Finished Training ...')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best test acc during training:  0.8802  epoch:  191\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"if model_version == 0:\n",
    "    PATH = 'C:\\\\Users\\Cena\\PycharmProjects\\csc413 final\\efficientnetb0.pt'\n",
    "    model = EfficientNetB0()\n",
    "else:\n",
    "    PATH = 'C:\\\\Users\\Cena\\PycharmProjects\\csc413 final\\efficientnetb0_0.pt'\n",
    "    model = EfficientNet.from_name('efficientnet-b0')\n",
    "\n",
    "state_dict = torch.load(PATH)\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model.load_state_dict(new_state_dict)\n",
    "net = model.cuda()\n",
    "net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\"\"\"\n",
    "\n",
    "\"\"\"correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        inputs = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images:',\n",
    "    correct / total)\"\"\"\n",
    "\n",
    "best_test_acc = max(test_acc_list)\n",
    "index = test_acc_list.index(max(test_acc_list)) + 1\n",
    "print(\"Best test acc during training: \", best_test_acc, \" epoch: \", index)\n",
    "\n",
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}